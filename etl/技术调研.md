# 数据处理调研

## 1. 当前主流数据处理组件


| 维度                   | DolphinScheduler   | Flink CDC       | SeaTunnel         | Dinky            | Kettle       | Apache NiFi    |
| ---------------------- | ------------------ | --------------- | ----------------- | ---------------- | ------------ | -------------- |
| **核心定位**     | 任务调度           | 实时数据同步    | 数据集成          | Flink 开发平台   | ETL 工具     | 数据流管理     |
| **架构扩展性**   | 分布式高可用       | 依赖 Flink      | 分布式/单机       | 依赖 Flink       | 单机         | 集群扩展       |
| **实时处理能力** | 弱                 | 强              | 强（Flink/Spark） | 强（Flink）      | 无           | 中（需自定义） |
| **开发友好性**   | 中（需配置工作流） | 低（需编程）    | 中（配置文件）    | 高（SQL 界面）   | 高（图形化） | 中（可视化流） |
| **社区生态**     | 活跃（Apache）     | 依赖 Flink 生态 | 快速成长          | 较小（国内主导） | 成熟         | 活跃（Apache） |
| **典型场景**     | 离线任务编排       | 数据库 CDC      | 批流数据同步      | Flink 作业管理   | 传统 ETL     | 数据路由与分发 |


## 2.基于场景的一些任务实现

### 场景一：实时同步A表的部分数据到多个目的库表中

**实现技术：Flink + Flink CDC + Java**

**核心代码：**

```java
public static void main(String[] args) throws Exception {
        // 加载配置文件
        Properties properties = new Properties();
        try (InputStream input = Mysql2Mysql.class.getClassLoader().getResourceAsStream("config.properties")) {
            if (input == null) {
                System.out.println("Sorry, unable to find config.properties");
                return;
            }
            properties.load(input);
        }

        // 设置 Flink 执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 配置 MySQL CDC Source
        MySqlSource<String> mySQLSource = MySqlSource.<String>builder()
                .hostname(properties.getProperty("mysql.hostname"))
                .port(Integer.parseInt(properties.getProperty("mysql.port")))
                .databaseList(properties.getProperty("mysql.database")) // 设置要监听的数据库
                .tableList(properties.getProperty("mysql.table")) // 设置要监听的表
                .username(properties.getProperty("mysql.username"))
                .password(properties.getProperty("mysql.password"))
                .serverTimeZone(properties.getProperty("mysql.serverTimeZone"))
                .deserializer(new JsonDebeziumDeserializationSchema()) // 使用 JSON 反序列化
                .startupOptions(StartupOptions.initial()) // 从初始快照开始
                .build();

        // 创建 CDC 数据流
        DataStreamSource<String> source = env.fromSource(mySQLSource, WatermarkStrategy.noWatermarks(),"Mysql Source");

        // 修改为返回UserOperation流
        DataStream<UserOperation> userOperationStream = source.map(new MapFunction<String, UserOperation>() {
            private final ObjectMapper objectMapper = new ObjectMapper();

            @Override
            public UserOperation map(String value) throws Exception {
                System.out.println("DEBUG value: " + value);
                logger.info("value:{}",value);
                JsonNode jsonNode = objectMapper.readTree(value);
                String op = jsonNode.get("op").asText(); // 获取操作类型
                JsonNode dataNode = null;

                // 处理删除事件使用before节点
                if ("d".equals(op)) {
                    dataNode = jsonNode.get("before");
                } else {
                    dataNode = jsonNode.get("after");
                }

                User user = new User();
                if (dataNode != null) {
                    user.setId(dataNode.get("id").asLong());
                    user.setName(dataNode.get("name").asText());
                    user.setAge(dataNode.get("age").asText());
                    user.setFrom(dataNode.get("from").asText());
                    user.setAreaCode(dataNode.get("area_code").asText());
                    user.setUserStatus(dataNode.get("user_status").asText());
                }
                return new UserOperation(op, user);
            }
        }).returns(TypeInformation.of(UserOperation.class));

        // 路由规则，根据ID的单数和双数分流
        DataStream<UserOperation> oddIdStream = userOperationStream.process(new ProcessFunction<UserOperation, UserOperation>() {
            @Override
            public void processElement(UserOperation value, Context ctx, Collector<UserOperation> out) throws Exception {
                if (value.getUser().getId() % 2 != 0) {
                    out.collect(value);
                }
            }
        });

        DataStream<UserOperation> evenIdStream = userOperationStream.process(new ProcessFunction<UserOperation, UserOperation>() {
            @Override
            public void processElement(UserOperation value, Context ctx, Collector<UserOperation> out) throws Exception {
                if (value.getUser().getId() % 2 == 0) {
                    out.collect(value);
                }
            }
        });

        // 使用自定义Sink处理不同操作
        oddIdStream.addSink(new CustomJdbcSink("config_odd.properties"))
                .name("Mysql Sink for Odd IDs");

        evenIdStream.addSink(new CustomJdbcSink("config_even.properties"))
                .name("Mysql Sink for Even IDs");

        // 启动 Flink 作业
        env.execute("MySQL CDC to MySQL Sync");
    }
```


### 场景二：脚本1的输出，作为脚本2的输入参数

**实现技术：Dolphinscheduler + Python**

**核心代码：**

```python
import json
import mysql.connector
# 解析如下数据：[{"id": 1, "age": "12", "from": "北京", "name": "小米", "areaCode": "110102","userStatus":"0"}]

# JSON data
data = '${result_json}'

# Parse JSON data
parsed_data = json.loads(data)

# 对数据进行遍历
for item in parsed_data:
    item['from'] = item['from'] + "_python"

# MySQL 配置信息
config = {
    'user': 'root',
    'password': 'root123',
    'host': '127.0.0.1',
    'database': 'test1'
}

# 与Mysql建立连接
conn = mysql.connector.connect(**config)
cursor = conn.cursor()

# 数据存储到Mysql
insert_query = '''
INSERT INTO t_user (id, age, `from`, name, area_code, user_status)
VALUES (%s, %s, %s, %s, %s, %s)
ON DUPLICATE KEY UPDATE
age = VALUES(age),
`from` = VALUES(`from`),
name = VALUES(name),
area_code = VALUES(area_code),
user_status = VALUES(user_status)
'''
for item in parsed_data:
    cursor.execute(insert_query, (item['id'], item['age'], item['from'], item['name'], item['areaCode'], item['userStatus']))

# 事务提交
conn.commit()

# 关闭连接
cursor.close()
conn.close()

```
